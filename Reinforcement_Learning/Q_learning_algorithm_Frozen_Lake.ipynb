{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "85cae898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "env = gym.make('FrozenLake-v1', is_slippery=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "729ae186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state, epsilon, i):\n",
    "    if random.uniform(0,1) < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        return np.argmax(Q[state,:] + np.random.randn(1,env.action_space.n)*epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "90aa2ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_learning_algorithm(alpha, gamma, epsilon, number_episodes):\n",
    "    total_rewards = []\n",
    "    Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    print(\"Q-Table before training\\n\")\n",
    "    print(Q, \"\\n\")\n",
    "    for i in range(number_episodes):\n",
    "        #On réinitialise l'environnement. On récupère le premier état.\n",
    "        state,_ = env.reset()\n",
    "        sum_reward = 0\n",
    "        done = False\n",
    "        #L'algorithme d'apprentissage de la Q-table\n",
    "        while True:\n",
    "            action = epsilon_greedy_policy(state, epsilon, i)\n",
    "            #On récupère un nouvel état et une nouvelle récompense de l'environment\n",
    "            state_next, reward, done, _, _ = env.step(action)\n",
    "            #On actualise la Q-table \n",
    "            Q[state,action] = Q[state,action] + alpha * (reward + gamma * np.max(Q[state_next,:]) - Q[state,action])\n",
    "            \n",
    "            #Si on a une récompense nous avons eu un succès   \n",
    "            sum_reward += reward\n",
    "            state = state_next\n",
    "            if done == True:\n",
    "                break   \n",
    "                \n",
    "        total_rewards.append(sum_reward)\n",
    "    return Q, total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "32ea9008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Table before training\n",
      "\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]] \n",
      "\n",
      "Q-Table after training \n",
      "\n",
      "[[0.45080908 0.45399731 0.43191475 0.45033646]\n",
      " [0.44107408 0.         0.37622538 0.41178619]\n",
      " [0.35434512 0.39160004 0.2990488  0.37291397]\n",
      " [0.31982313 0.         0.23198409 0.25089438]\n",
      " [0.44561586 0.45651943 0.         0.442965  ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.41476251 0.         0.29336956]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.40777073 0.         0.46994836 0.4196112 ]\n",
      " [0.3306548  0.52932674 0.43423655 0.        ]\n",
      " [0.25925306 0.62061645 0.         0.21159674]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.41274568 0.62612347 0.32647724]\n",
      " [0.30134786 0.6138046  0.81469798 0.377462  ]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "Q_table, final_reward = Q_learning_algorithm(alpha = 0.1, gamma = 0.9999, epsilon = 0.015, number_episodes = 1000)\n",
    "print (\"Q-Table after training \\n\")\n",
    "print (Q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "bdb8be85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate = 100.0%\n"
     ]
    }
   ],
   "source": [
    "episodes = 100\n",
    "nb_success = 0\n",
    "\n",
    "# Test\n",
    "for _ in range(episodes):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        #Choisir l'action dont la valeur est la plus élevée dans l'état actuel.\n",
    "        action = np.argmax(Q_table[state])\n",
    "\n",
    "        #Exécuter cette action et déplacer l'agent dans la direction souhaitée\n",
    "        new_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        #Mettre à jour notre état actuel\n",
    "        state = new_state\n",
    "\n",
    "        #Quand on obtient une récompense, cela signifie qu'on a résolu le jeu\n",
    "        nb_success += reward\n",
    "\n",
    "#Taux de succès\n",
    "print (f\"Success rate = {nb_success/episodes*100}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
